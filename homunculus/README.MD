# Degrees of Seperation Between Wikipedia Articles; How many click's away from the Homunculus?

The file `wikigraph.py` implements classes for finding paths between wikipedia articles and other related functions using the [wikimedia API](https://www.mediawiki.org/wiki/API:Main_page). A path is created by linking articles by the links they contain, just like the [wikipedia game](https://en.wikipedia.org/wiki/Wikipedia:Wiki_Game). Here it does this using a bi-directional search to find the first path it can (this is not necessarily the shortest, the use of bi-directional was a trade-off to increase speed and minimize api requests).

## Usage:

The `WikiGraph.find_path` method is better run in a shell
session or in a batch collection as its use of memoization will
speed up searches whilst it runs, reducing requests to the Wikimedia API.

### Example session

    >>> from wikigraph import WikiGraph
    >>> wg = WikiGraph()
    >>> path = wg.find_path("Kevin Bacon", "Tom Hanks")
    >>> path.print_stats()
    Found Path:
        Path:        Tom Hanks -> Kevin Bacon
        Separation:  1 steps
        Time Taken:  1.498983 seconds
        Requests:    5
    -------------------------------------------------------------
    >>> path.data()
    {'start': 'Kevin Bacon', 'end': 'Tom Hanks', 'path': 'Tom Hanks->Kevin Bacon', 'degree': 1}

### collectbatch.py

For a given sample of start articles find a path from each to a central end article.
Save the output to a given csv file. Without start list specified, program
will default to collecting an `n` sized [random sample](https://www.mediawiki.org/wiki/API:Random)generated by the wikimedia API. For more info, See command line arg details below.

    usage: collectbatch.py [-h] [-x CENTER] [-s SOURCE] [-n NUM] outfile

    positional arguments:
    outfile               Filename to save the results to.

    optional arguments:
    -h, --help            show this help message and exit
    -x CENTER, --center CENTER
                            Title of valid wiki page to center all nodes from.
                            default="Homunculus"
    -s SOURCE, --source SOURCE
                            Filename containing newline delimited list of valid
                            wiki article titles if not specified sample defaults
                            to random selection from wikimedia api.
    -n NUM, --num NUM     Sample size to collect. default=1.

**Requirements:** `requests`